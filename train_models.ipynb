{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple model\n",
    "\n",
    "This is the code to train a simple model using keras. This seems to be unefficient, making a very complex and big CNN. However with no no more then 3500 images it converges well to almost 100% accuracy (on evaluation). Still only using, one model per person. Using only one type of attack.\n",
    "\n",
    "The need of a more robust method would be good. A model that can generilize from multiple people and corectly discriminate live faces from spoofed ones, form different attacks. That would require more and better models of style tranfer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras import callbacks\n",
    "from keras.regularizers import l2\n",
    "import pickle\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os\n",
    "now_datetime  = datetime.datetime.now()\n",
    "NAME = f\"#Face_spoofing300x2_{now_datetime.day:02d}{now_datetime.month:02d}{now_datetime.year}_{now_datetime.hour:02d}{now_datetime.minute:02d}\"\n",
    "dir_pickle = \"database_serialized\"\n",
    "dir_models_save = \"models\"\n",
    "person = \"001\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model format\n",
    "This is a simple CNN, that is made of a convolutional and a dense layer. below you choose the numbers of filters of each concolutional layer, and the numbers of neurons of each dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#format of the convolutional layer\n",
    "format_convolution = [30,60,200]\n",
    "#format of the dense layer\n",
    "format_dense = [20,30,30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model\n",
    "The model creaeted here is enought for an almost ideal (acc = 98%) spoof detection model, as expected from a liveness detection methods that include deep learning as shown on many [studies](https://www.emerald.com/insight/content/doi/10.1108/SR-08-2015-0136/full/html) before.\n",
    "\n",
    "However the artifical creation of spoofed images, may reduce its performance. For 2 different reasons:\n",
    "1. Non perfect spoofed images. The creating of a model that can generate an spoofed image relayes much on the single image and the trainning data used to create such model. If both are not manage correctly, the creation of bad models may occur. Not to mention that some types of attacks are really hard to imitate e.g. print attacks.\n",
    "2. Reduced numbers of data. There isn't much data online for this type of problem, and it to the creation of individuals models for each person may prove unresenable, for each person must have minutes (about two) of video of their faces, for a better conversion of the CNN.\n",
    "\n",
    "For those reasons, it would be needed a deep leraning method that requires less data, to generalize. Something that could be accuired by [this](https://www.sciencedirect.com/science/article/pii/S1047320318301044) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(person, pickle_path):\n",
    "    pickle_in = open(os.path.join(pickle_path,f\"X{person}.pickle\"),\"rb\")\n",
    "    X = pickle.load(pickle_in)\n",
    "    pickle_in.close()\n",
    "    pickle_in = open(os.path.join(pickle_path,f\"y{person}.pickle\"),\"rb\")\n",
    "    y = pickle.load(pickle_in)\n",
    "    pickle_in.close()\n",
    "    X = X/255.0\n",
    "    model = Sequential()\n",
    "    is_first = True\n",
    "    # Convolutional layers\n",
    "    for format_c in format_convolution:\n",
    "        if is_first:\n",
    "            model.add(layers.Conv2D(format_c,(3,3),input_shape=X.shape[1:]))\n",
    "            is_first = False\n",
    "        else:\n",
    "            model.add(layers.Conv2D(format_c,(3,3)))\n",
    "        model.add(layers.Activation(\"relu\"))\n",
    "        model.add(layers.MaxPool2D(pool_size=(3,3)))\n",
    "    #Flatten the model if needed\n",
    "    model.add(layers.Flatten())\n",
    "    \n",
    "    #Dense layers\n",
    "    for format_d in format_dense:\n",
    "        model.add(layers.Dropout(0.12))\n",
    "        model.add(layers.Dense(format_d,activation=\"relu\",kernel_regularizer=l2(0.002)))\n",
    "    \n",
    "    #Output layer\n",
    "    model.add(layers.Dense(1,activation=\"sigmoid\"))\n",
    "    \n",
    "    \n",
    "    model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer = \"adam\",\n",
    "              metrics= [\"accuracy\"])\n",
    "    \n",
    "    return X, y, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1007 15:07:19.087997 15876 deprecation.py:506] From E:\\Anaconda3\\envs\\face_spoofing\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W1007 15:07:19.401949 15876 deprecation.py:323] From E:\\Anaconda3\\envs\\face_spoofing\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "# Create the model structure for the person\n",
    "X, y, model = create_model(person,os.path.join(dir_pickle,person))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainning your model\n",
    "This line trains your model. It is recoomend to use tensorflow-gpu, and to be carefull if your gpu can handle the data, if  you are having *ResourceExhaustedError*, try reducing the **batch size**, or the sizes of your **tesors**.\n",
    "\n",
    "Keep the eye on your **validation** because that shows your live performance without trainning bias. You may use checkpoints to get the best validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard = callbacks.TensorBoard(log_dir=os.path.join(\"tensorBoard_logs\",NAME),write_graph=True,update_freq=\"batch\",\n",
    "                                    batch_size=5)\n",
    "checkpoints = callbacks.ModelCheckpoint(filepath=os.path.join(dir_models_save,f\"{person}{NAME}{format_convolution}{format_dense}.h5\"),\n",
    "                                        monitor=\"val_acc\",\n",
    "                                        mode = \"max\",\n",
    "                                        verbose = 1,\n",
    "                                        save_weights_only=False,\n",
    "                                        save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2942 samples, validate on 603 samples\n",
      "Epoch 1/7\n",
      "2925/2942 [============================>.] - ETA: 0s - loss: 0.3199 - acc: 0.9032\n",
      "Epoch 00001: val_acc improved from -inf to 1.00000, saving model to models\\001#Face_spoofing300x2_07102019_1507[30, 60, 200][20, 30, 30].h5\n",
      "2942/2942 [==============================] - 33s 11ms/sample - loss: 0.3186 - acc: 0.9038 - val_loss: 0.0901 - val_acc: 1.0000\n",
      "Epoch 2/7\n",
      "2925/2942 [============================>.] - ETA: 0s - loss: 0.0869 - acc: 0.9973\n",
      "Epoch 00002: val_acc did not improve from 1.00000\n",
      "2942/2942 [==============================] - 42s 14ms/sample - loss: 0.0868 - acc: 0.9973 - val_loss: 0.0678 - val_acc: 1.0000\n",
      "Epoch 3/7\n",
      "2925/2942 [============================>.] - ETA: 0s - loss: 0.0795 - acc: 0.9887\n",
      "Epoch 00003: val_acc did not improve from 1.00000\n",
      "2942/2942 [==============================] - 43s 15ms/sample - loss: 0.0793 - acc: 0.9888 - val_loss: 0.0540 - val_acc: 1.0000\n",
      "Epoch 4/7\n",
      "2925/2942 [============================>.] - ETA: 0s - loss: 0.0587 - acc: 0.9918\n",
      "Epoch 00004: val_acc did not improve from 1.00000\n",
      "2942/2942 [==============================] - 42s 14ms/sample - loss: 0.0588 - acc: 0.9915 - val_loss: 0.0403 - val_acc: 1.0000\n",
      "Epoch 5/7\n",
      "2925/2942 [============================>.] - ETA: 0s - loss: 0.0448 - acc: 0.9942\n",
      "Epoch 00005: val_acc did not improve from 1.00000\n",
      "2942/2942 [==============================] - 43s 15ms/sample - loss: 0.0448 - acc: 0.9942 - val_loss: 0.0322 - val_acc: 1.0000\n",
      "Epoch 6/7\n",
      "2925/2942 [============================>.] - ETA: 0s - loss: 0.0393 - acc: 0.9935\n",
      "Epoch 00006: val_acc did not improve from 1.00000\n",
      "2942/2942 [==============================] - 43s 15ms/sample - loss: 0.0395 - acc: 0.9932 - val_loss: 0.0252 - val_acc: 1.0000\n",
      "Epoch 7/7\n",
      "2925/2942 [============================>.] - ETA: 0s - loss: 0.0313 - acc: 0.9952\n",
      "Epoch 00007: val_acc did not improve from 1.00000\n",
      "2942/2942 [==============================] - 25s 8ms/sample - loss: 0.0312 - acc: 0.9952 - val_loss: 0.0216 - val_acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x25301965e48>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train model\n",
    "model.fit(X,y,batch_size=25,validation_split=0.17,epochs=7,shuffle=False, callbacks=[checkpoints])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os._exit(00)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "face_spoofing",
   "language": "python",
   "name": "face_spoofing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
