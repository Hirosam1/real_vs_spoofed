{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple model\n",
    "\n",
    "This is the code to train a simple model using keras. This seems to be unefficient, making a very complex and big CNN. However with no no more then 3500 images it converges well to almost 100% accuracy (on evaluation). Still only using, one model per person. Using only one type of attack.\n",
    "\n",
    "The need of a more robust method would be good. A model that can generilize from multiple people and corectly discriminate live faces from spoofed ones, form different attacks. That would require more and better models of style tranfer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras import callbacks\n",
    "from keras.regularizers import l2\n",
    "import pickle\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os\n",
    "now_datetime  = datetime.datetime.now()\n",
    "NAME = f\"#Face_spoofing300x2_{now_datetime.day:02d}{now_datetime.month:02d}{now_datetime.year}_{now_datetime.hour:02d}{now_datetime.minute:02d}\"\n",
    "dir_pickle = \"database_serialized\"\n",
    "dir_models_save = \"models\"\n",
    "person = \"001Aug\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model format\n",
    "This is a simple CNN, that is made of a convolutional and a dense layer. below you choose the numbers of filters of each concolutional layer, and the numbers of neurons of each dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#format of the convolutional layer\n",
    "format_convolution = [80,140,320]\n",
    "#format of the dense layer\n",
    "format_dense = [400,300,200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model\n",
    "The model creaeted here is enought for an almost ideal (acc = 98%) spoof detection model, as expected from a liveness detection methods that include deep learning as shown on many [studies](https://www.emerald.com/insight/content/doi/10.1108/SR-08-2015-0136/full/html) before.\n",
    "\n",
    "However the artifical creation of spoofed images, may reduce its performance. For 2 different reasons:\n",
    "1. Non perfect spoofed images. The creating of a model that can generate an spoofed image relayes much on the single image and the trainning data used to create such model. If both are not manage correctly, the creation of bad models may occur. Not to mention that some types of attacks are really hard to imitate e.g. print attacks.\n",
    "2. Reduced numbers of data. There isn't much data online for this type of problem, and it to the creation of individuals models for each person may prove unresenable, for each person must have minutes (about two) of video of their faces, for a better conversion of the CNN.\n",
    "\n",
    "For those reasons, it would be needed a deep leraning method that requires less data, to generalize. Something that could be accuired by [this](https://www.sciencedirect.com/science/article/pii/S1047320318301044) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(person, pickle_path):\n",
    "    pickle_in = open(os.path.join(pickle_path,f\"X{person}.pickle\"),\"rb\")\n",
    "    X = pickle.load(pickle_in)\n",
    "    pickle_in.close()\n",
    "    pickle_in = open(os.path.join(pickle_path,f\"y{person}.pickle\"),\"rb\")\n",
    "    y = pickle.load(pickle_in)\n",
    "    pickle_in.close()\n",
    "    X = X/255.0\n",
    "    model = Sequential()\n",
    "    is_first = True\n",
    "    # Convolutional layers\n",
    "    for format_c in format_convolution:\n",
    "        if is_first:\n",
    "            model.add(layers.Conv2D(format_c,(3,3),input_shape=X.shape[1:]))\n",
    "            is_first = False\n",
    "        else:\n",
    "            model.add(layers.Conv2D(format_c,(3,3)))\n",
    "        model.add(layers.Activation(\"relu\"))\n",
    "        model.add(layers.MaxPool2D(pool_size=(3,3)))\n",
    "    #Flatten the model if needed\n",
    "    model.add(layers.Flatten())\n",
    "    \n",
    "    #Dense layers\n",
    "    for format_d in format_dense:\n",
    "        model.add(layers.Dropout(0.12))\n",
    "        model.add(layers.Dense(format_d,activation=\"relu\",kernel_regularizer=l2(0.002)))\n",
    "    \n",
    "    #Output layer\n",
    "    model.add(layers.Dense(1,activation=\"sigmoid\"))\n",
    "    \n",
    "    \n",
    "    model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer = \"adam\",\n",
    "              metrics= [\"accuracy\"])\n",
    "    \n",
    "    return X, y, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1007 22:10:47.563754  3792 deprecation.py:506] From E:\\Anaconda3\\envs\\face_spoofing\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W1007 22:10:48.075386  3792 deprecation.py:323] From E:\\Anaconda3\\envs\\face_spoofing\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "# Create the model structure for the person\n",
    "X, y, model = create_model(person,os.path.join(dir_pickle,person))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainning your model\n",
    "This line trains your model. It is recoomend to use tensorflow-gpu, and to be carefull if your gpu can handle the data, if  you are having *ResourceExhaustedError*, try reducing the **batch size**, or the sizes of your **tesors**.\n",
    "\n",
    "Keep the eye on your **validation** because that shows your live performance without trainning bias. You may use checkpoints to get the best validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard = callbacks.TensorBoard(log_dir=os.path.join(\"tensorBoard_logs\",NAME),write_graph=True,update_freq=\"batch\",\n",
    "                                    batch_size=5)\n",
    "if not os.path.isdir(os.path.join(dir_models_save,person)):\n",
    "    os.mkdir(os.path.join(dir_models_save,person))\n",
    "checkpoints = callbacks.ModelCheckpoint(filepath=os.path.join(dir_models_save,person,f\"{person}{NAME}{format_convolution}{format_dense}.h5\"),\n",
    "                                        monitor=\"val_acc\",\n",
    "                                        mode = \"max\",\n",
    "                                        verbose = 1,\n",
    "                                        save_weights_only=False,\n",
    "                                        save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3329 samples, validate on 588 samples\n",
      "Epoch 1/7\n",
      "3325/3329 [============================>.] - ETA: 0s - loss: 0.4807 - acc: 0.9474\n",
      "Epoch 00001: val_acc improved from -inf to 1.00000, saving model to models\\001Aug\\001Aug#Face_spoofing300x2_07102019_2210[80, 140, 320][400, 300, 200].h5\n",
      "3329/3329 [==============================] - 61s 18ms/sample - loss: 0.4803 - acc: 0.9474 - val_loss: 0.1386 - val_acc: 1.0000\n",
      "Epoch 2/7\n",
      "3325/3329 [============================>.] - ETA: 0s - loss: 0.0938 - acc: 0.9976\n",
      "Epoch 00002: val_acc did not improve from 1.00000\n",
      "3329/3329 [==============================] - 56s 17ms/sample - loss: 0.0938 - acc: 0.9976 - val_loss: 0.0418 - val_acc: 1.0000\n",
      "Epoch 3/7\n",
      "3325/3329 [============================>.] - ETA: 0s - loss: 0.0602 - acc: 0.9964\n",
      "Epoch 00003: val_acc did not improve from 1.00000\n",
      "3329/3329 [==============================] - 56s 17ms/sample - loss: 0.0602 - acc: 0.9964 - val_loss: 0.0416 - val_acc: 1.0000\n",
      "Epoch 4/7\n",
      "3325/3329 [============================>.] - ETA: 0s - loss: 0.0564 - acc: 0.9952\n",
      "Epoch 00004: val_acc did not improve from 1.00000\n",
      "3329/3329 [==============================] - 56s 17ms/sample - loss: 0.0564 - acc: 0.9952 - val_loss: 0.0410 - val_acc: 1.0000\n",
      "Epoch 5/7\n",
      "1225/3329 [==========>...................] - ETA: 33s - loss: 0.0307 - acc: 1.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-714ae5e9c288>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Train model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.15\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcheckpoints\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mE:\\Anaconda3\\envs\\face_spoofing\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    778\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m           \u001b[0mvalidation_freq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m           steps_name='steps_per_epoch')\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\face_spoofing\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m         \u001b[1;31m# Get outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 363\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\face_spoofing\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3292\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\face_spoofing\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1458\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1459\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Train model\n",
    "model.fit(X,y,batch_size=7,validation_split=0.15,epochs=7,shuffle=False, callbacks=[checkpoints])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kills the kernel to free-up memory on GPU, also avoiding collisions with othhers scripts \n",
    "#comment the line below, if you want to keep the variables and buffers\n",
    "os._exit(00)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "face_spoofing",
   "language": "python",
   "name": "face_spoofing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
