{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple model\n",
    "\n",
    "This is the code to train a simple model using keras. This seems to be unefficient, making a very complex and big CNN. However with no no more then 3500 images it converges well to almost 100% accuracy (on evaluation). Still only using, one model per person. Using only one type of attack.\n",
    "\n",
    "The need of a more robust method would be good. A model that can generilize from multiple people and corectly discriminate live faces from spoofed ones, form different attacks. That would require more and better models of style tranfer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('64bit', 'WindowsPE')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras import callbacks\n",
    "from keras.regularizers import l2\n",
    "import pickle\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os\n",
    "now_datetime  = datetime.datetime.now()\n",
    "NAME = f\"#Face_spoofing300x2_{now_datetime.day:02d}{now_datetime.month:02d}{now_datetime.year}_{now_datetime.hour:02d}{now_datetime.minute:02d}\"\n",
    "dir_pickle = \"database_serialized\"\n",
    "dir_models_save = \"models\"\n",
    "person = \"001\"\n",
    "import platform\n",
    "print(platform.architecture())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model format\n",
    "This is a simple CNN, that is made of a convolutional and a dense layer. below you choose the numbers of filters of each concolutional layer, and the numbers of neurons of each dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#format of the convolutional layer\n",
    "format_convolution = [80,140,320]\n",
    "#format of the dense layer\n",
    "format_dense = [400,300,200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model\n",
    "The model creaeted here is enought for an almost ideal (acc = 98%) spoof detection model, as expected from a liveness detection methods that include deep learning as shown on many [studies](https://www.emerald.com/insight/content/doi/10.1108/SR-08-2015-0136/full/html) before.\n",
    "\n",
    "However the artifical creation of spoofed images, may reduce its performance. For 2 different reasons:\n",
    "1. Non perfect spoofed images. The creating of a model that can generate an spoofed image relayes much on the single image and the trainning data used to create such model. If both are not manage correctly, the creation of bad models may occur. Not to mention that some types of attacks are really hard to imitate e.g. print attacks.\n",
    "2. Reduced numbers of data. There isn't much data online for this type of problem, and it to the creation of individuals models for each person may prove unresenable, for each person must have minutes (about two) of video of their faces, for a better conversion of the CNN.\n",
    "\n",
    "For those reasons, it would be needed a deep leraning method that requires less data, to generalize. Something that could be accuired by [this](https://www.sciencedirect.com/science/article/pii/S1047320318301044) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(person, pickle_path):\n",
    "    pickle_in = open(os.path.join(pickle_path,f\"X{person}.pickle\"),\"rb\")\n",
    "    X = pickle.load(pickle_in)\n",
    "    pickle_in.close()\n",
    "    pickle_in = open(os.path.join(pickle_path,f\"y{person}.pickle\"),\"rb\")\n",
    "    y = pickle.load(pickle_in)\n",
    "    pickle_in.close()\n",
    "    \n",
    "    X = X.astype(np.float16)/255.0\n",
    "    model = Sequential()\n",
    "    is_first = True\n",
    "    # Convolutional layers\n",
    "    for format_c in format_convolution:\n",
    "        if is_first:\n",
    "            model.add(layers.Conv2D(format_c,(3,3),input_shape=X.shape[1:]))\n",
    "            is_first = False\n",
    "        else:\n",
    "            model.add(layers.Conv2D(format_c,(3,3)))\n",
    "        model.add(layers.Activation(\"relu\"))\n",
    "        model.add(layers.MaxPool2D(pool_size=(3,3)))\n",
    "    #Flatten the model if needed\n",
    "    model.add(layers.Flatten())\n",
    "    \n",
    "    #Dense layers\n",
    "    for format_d in format_dense:\n",
    "        model.add(layers.Dropout(0.12))\n",
    "        model.add(layers.Dense(format_d,activation=\"relu\",kernel_regularizer=l2(0.002)))\n",
    "    \n",
    "    #Output layer\n",
    "    model.add(layers.Dense(1,activation=\"sigmoid\"))\n",
    "    \n",
    "    \n",
    "    model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer = \"adam\",\n",
    "              metrics= [\"accuracy\"])\n",
    "    \n",
    "    return X, y, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\Anaconda3\\envs\\face_spoofing\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From E:\\Anaconda3\\envs\\face_spoofing\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "# Create the model structure for the person\n",
    "X, y, model = create_model(person,os.path.join(dir_pickle,person))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainning your model\n",
    "This line trains your model. It is recoomend to use tensorflow-gpu, and to be carefull if your gpu can handle the data, if  you are having *ResourceExhaustedError*, try reducing the **batch size**, or the sizes of your **tesors**.\n",
    "\n",
    "Keep the eye on your **validation** because that shows your live performance without trainning bias. You may use checkpoints to get the best validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard = callbacks.TensorBoard(log_dir=os.path.join(\"tensorBoard_logs\",NAME),write_graph=True,update_freq=\"batch\",\n",
    "                                    batch_size=5)\n",
    "if not os.path.isdir(os.path.join(dir_models_save,person)):\n",
    "    os.mkdir(os.path.join(dir_models_save,person))\n",
    "checkpoints = callbacks.ModelCheckpoint(filepath=os.path.join(dir_models_save,person,f\"{person}{NAME}{format_convolution}{format_dense}.h5\"),\n",
    "                                        monitor=\"val_acc\",\n",
    "                                        mode = \"max\",\n",
    "                                        verbose = 1,\n",
    "                                        save_weights_only=False,\n",
    "                                        save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9039 samples, validate on 1596 samples\n",
      "Epoch 1/7\n",
      "9037/9039 [============================>.] - ETA: 0s - loss: 0.3086 - acc: 0.9878\n",
      "Epoch 00001: val_acc improved from -inf to 1.00000, saving model to models\\001\\001#Face_spoofing300x2_11102019_1615[80, 140, 320][400, 300, 200].h5\n",
      "9039/9039 [==============================] - 164s 18ms/sample - loss: 0.3086 - acc: 0.9878 - val_loss: 0.0714 - val_acc: 1.0000\n",
      "Epoch 2/7\n",
      "9037/9039 [============================>.] - ETA: 0s - loss: 0.1404 - acc: 0.9952\n",
      "Epoch 00002: val_acc did not improve from 1.00000\n",
      "9039/9039 [==============================] - 152s 17ms/sample - loss: 0.1404 - acc: 0.9952 - val_loss: 0.0319 - val_acc: 1.0000\n",
      "Epoch 3/7\n",
      "9037/9039 [============================>.] - ETA: 0s - loss: 0.0448 - acc: 0.9991\n",
      "Epoch 00003: val_acc did not improve from 1.00000\n",
      "9039/9039 [==============================] - 161s 18ms/sample - loss: 0.0447 - acc: 0.9991 - val_loss: 0.0204 - val_acc: 1.0000\n",
      "Epoch 4/7\n",
      "9037/9039 [============================>.] - ETA: 0s - loss: 0.0443 - acc: 0.9983\n",
      "Epoch 00004: val_acc did not improve from 1.00000\n",
      "9039/9039 [==============================] - 154s 17ms/sample - loss: 0.0443 - acc: 0.9983 - val_loss: 0.0744 - val_acc: 0.9987\n",
      "Epoch 5/7\n",
      "9037/9039 [============================>.] - ETA: 0s - loss: 0.0535 - acc: 0.9968\n",
      "Epoch 00005: val_acc did not improve from 1.00000\n",
      "9039/9039 [==============================] - 149s 17ms/sample - loss: 0.0535 - acc: 0.9968 - val_loss: 0.0606 - val_acc: 1.0000\n",
      "Epoch 6/7\n",
      "9037/9039 [============================>.] - ETA: 0s - loss: 0.0411 - acc: 0.9990\n",
      "Epoch 00006: val_acc did not improve from 1.00000\n",
      "9039/9039 [==============================] - 150s 17ms/sample - loss: 0.0411 - acc: 0.9990 - val_loss: 0.0160 - val_acc: 1.0000\n",
      "Epoch 7/7\n",
      "9037/9039 [============================>.] - ETA: 0s - loss: 0.0086 - acc: 1.0000\n",
      "Epoch 00007: val_acc did not improve from 1.00000\n",
      "9039/9039 [==============================] - 149s 17ms/sample - loss: 0.0086 - acc: 1.0000 - val_loss: 0.0090 - val_acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1c9e66f5bc8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train model\n",
    "model.fit(X,y,batch_size=7,validation_split=0.15,epochs=7,shuffle=False, callbacks=[checkpoints])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kills the kernel to free-up memory on GPU, also avoiding collisions with othhers scripts \n",
    "#comment the line below, if you want to keep the variables and buffers\n",
    "os._exit(00)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "face_spoofing",
   "language": "python",
   "name": "face_spoofing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
